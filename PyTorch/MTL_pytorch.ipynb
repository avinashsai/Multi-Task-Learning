{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "MTL pytorch.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "machine_shape": "hm"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "MZpVlH4JD48u",
        "colab_type": "code",
        "outputId": "297689f9-b7b4-4cc1-8659-cc9bed414c1e",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 68
        }
      },
      "source": [
        "import os\n",
        "import re\n",
        "import sys\n",
        "import time\n",
        "import math\n",
        "import string\n",
        "import random\n",
        "import collections\n",
        "from collections import Counter,OrderedDict\n",
        "from copy import deepcopy\n",
        "import pickle\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import sklearn\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn.metrics import accuracy_score,f1_score\n",
        "from sklearn.model_selection import train_test_split,StratifiedKFold\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import torch.distributions as tdist\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "seed = 0\n",
        "#torch.autograd.set_detect_anomaly(False)\n",
        "import nltk\n",
        "from nltk.corpus import stopwords\n",
        "stopword = stopwords.words('english')"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "cuda\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kOmoVEW7FyOL",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "datapath = 'mtl/'\n",
        "vecpath = 'glove.840B.300d.txt'"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4YY3DIzmQPT5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!wget https://nlp.stanford.edu/data/glove.840B.300d.zip\n",
        "!unzip glove.840B.300d.zip\n",
        "!rm -rf glove.840B.30d.zip\n",
        "\n",
        "embedding_index = {}\n",
        "with open(vecpath,'r',encoding='utf-8') as f:\n",
        "    for line in f.readlines():\n",
        "      words = line.split(' ')\n",
        "      word = words[0]\n",
        "      v = torch.from_numpy(np.asarray([w for w in words[1:]],'float32'))\n",
        "      embedding_index[word] = v"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "y_voawr1EDxH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domainbatchsize = 2\n",
        "batchsize = 16\n",
        "fullyhidden1 = 300"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2qQ77-lSEFwu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domains = os.listdir(datapath)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-kxaUh1kEF0T",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "  def clean_str(sent):\n",
        "    sent = str(sent)\n",
        "    sent = re.sub(r\"[^A-Za-z0-9(),!?\\'\\`]\", \" \", sent)     \n",
        "    sent = re.sub(r\"\\'s\", \" \\'s\", sent) \n",
        "    sent = re.sub(r\"\\'ve\", \" \\'ve\", sent) \n",
        "    sent = re.sub(r\"n\\'t\", \" n\\'t\", sent) \n",
        "    sent = re.sub(r\"\\'re\", \" \\'re\", sent) \n",
        "    sent = re.sub(r\"\\'d\", \" \\'d\", sent) \n",
        "    sent = re.sub(r\"\\'ll\", \" \\'ll\", sent) \n",
        "    sent = re.sub(r\",\", \" , \", sent) \n",
        "    sent = re.sub(r\"!\", \" ! \", sent) \n",
        "    sent = re.sub(r\"\\(\", \" \\( \", sent) \n",
        "    sent = re.sub(r\"\\)\", \" \\) \", sent) \n",
        "    sent = re.sub(r\"\\?\", \" \\? \", sent) \n",
        "    sent = re.sub(r\"\\s{2,}\", \" \", sent)    \n",
        "    sent = re.sub(r\"[^a-zA-Z]\",\" \",sent)\n",
        "    sents = sent.split()\n",
        "    return \" \".join(word.lower() for word in sents if word not in stopword and len(word)>1)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QyYskGIOEF3W",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def preprocess(corpus):\n",
        "    return [clean_str(sentence) for sentence in corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LsphVHLgEF9g",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "domains_train_corpus = []\n",
        "domains_train_labels = []\n",
        "domains_val_corpus = []\n",
        "domains_val_labels = []\n",
        "domains_test_corpus = []\n",
        "domains_test_labels = []\n",
        "for domain in domains:\n",
        "    train = pd.read_csv(datapath+domain+'/train.csv')\n",
        "    domains_train_labels.append(train['label'].astype(int).values)\n",
        "    domains_train_corpus.append(preprocess(list(train['text'])))\n",
        "    \n",
        "    val = pd.read_csv(datapath+domain+'/val.csv')\n",
        "    domains_val_labels.append(val['label'].astype(int).values)\n",
        "    domains_val_corpus.append(preprocess(list(val['text'])))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-XOkFmv6EGAo",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "traindomainsize = [len(d) for d in domains_train_corpus]\n",
        "valdomainsize = [len(d) for d in domains_val_corpus]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "gy8sboN3EGEA",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "for i in range(len(domains)):\n",
        "    assert(len(domains_train_corpus[i])==len(domains_train_labels[i]))\n",
        "    assert(len(domains_val_corpus[i])==len(domains_val_labels[i]))\n",
        "    assert(len(domains_test_corpus[i])==len(domains_test_labels[i]))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GVAZqUuuEGKC",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "maxlen = 300\n",
        "embeddim = 300\n",
        "hiddendim = 128"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yEywe2HyEGNU",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vectors(sentence,vocabulary):\n",
        "    temp = [vocabulary[word] for word in sentence.split() if word in vocabulary]\n",
        "    vector = [0] * maxlen\n",
        "    curlen = len(temp)\n",
        "    if(maxlen-curlen<0):\n",
        "        vector = temp[:maxlen]\n",
        "    else:\n",
        "        vector[maxlen-curlen:] = temp\n",
        "\n",
        "    return torch.from_numpy(np.asarray(vector,dtype='int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j-OX3wMoEGTH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_vocab(data):\n",
        "    words = []\n",
        "    for sentence in data:\n",
        "        words+=sentence.split()\n",
        "\n",
        "    counts = Counter(words).most_common()\n",
        "    counts.insert(0,('<PAD>',0))\n",
        "    vocabulary = {word:i for i,(word,_) in enumerate(counts)}\n",
        "    return vocabulary"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DrkCI9sSEGcu",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_data(data,vocab):\n",
        "    vectors = torch.zeros(len(data),maxlen)\n",
        "    for i in range(len(data)):\n",
        "        vectors[i] = get_vectors(data[i],vocab)\n",
        "    return vectors"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2KcVpq1JEo8C",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "totaldata = []\n",
        "for d in domains_train_corpus:\n",
        "    totaldata.extend(d)\n",
        "vocabulary = get_vocab(totaldata)\n",
        "\n",
        "embedding_matrix = torch.zeros(len(vocabulary),embeddim).to(device)\n",
        "for i,word in enumerate(vocabulary):\n",
        "    vec = embedding_index.get(word)\n",
        "    if(vec is not None):\n",
        "        embedding_matrix[i] = vec[:embeddim]"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "978FsxqwzzBJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "0ddbadfd-31bc-4870-c1a1-d82b498e6db2"
      },
      "source": [
        "len(vocabulary)"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "55521"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 17
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UqzHnFnuEo_Z",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_vectors = [get_data(d,vocabulary) for d in domains_train_corpus]\n",
        "val_vectors = [get_data(d,vocabulary) for d in domains_val_corpus]   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Xg-TsnQoEpFS",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "trainlabels = []\n",
        "for i,d in enumerate(domains_train_corpus):\n",
        "    trainlabels.append([i]*len(d))\n",
        "\n",
        "vallabels = []\n",
        "for i,d in enumerate(domains_val_corpus):\n",
        "    vallabels.append([i]*len(d))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RMCAPtSWEpI7",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_domain_indices = []\n",
        "for i in range(max(traindomainsize)//domainbatchsize):\n",
        "    for j in range(len(traindomainsize)):\n",
        "        if((i+1)*domainbatchsize>traindomainsize[j]):\n",
        "            train_domain_indices.extend(train_vectors[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            train_domain_indices.extend(train_vectors[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "train_domain_labels = []\n",
        "for i in range(max(traindomainsize)//domainbatchsize):\n",
        "    for j in range(len(traindomainsize)):\n",
        "        if((i+1)*domainbatchsize>traindomainsize[j]):\n",
        "            train_domain_labels.extend(trainlabels[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            train_domain_labels.extend(trainlabels[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "train_sentiment_labels = []\n",
        "for i in range(max(traindomainsize)//domainbatchsize):\n",
        "    for j in range(len(traindomainsize)):\n",
        "        if((i+1)*domainbatchsize>traindomainsize[j]):\n",
        "            train_sentiment_labels.extend(domains_train_labels[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            train_sentiment_labels.extend(domains_train_labels[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "train_domain_indices = torch.stack([x for x in train_domain_indices])\n",
        "train_domain_labels = torch.from_numpy(np.asarray(train_domain_labels,'int32'))\n",
        "train_sentiment_labels = torch.from_numpy(np.asarray(train_sentiment_labels,'int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CzkfVSoNEpL9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "val_domain_indices = []\n",
        "for i in range(max(valdomainsize)//domainbatchsize):\n",
        "    for j in range(len(valdomainsize)):\n",
        "        if((i+1)*domainbatchsize>valdomainsize[j]):\n",
        "            val_domain_indices.extend(val_vectors[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            val_domain_indices.extend(val_vectors[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "val_domain_labels = []\n",
        "for i in range(max(valdomainsize)//domainbatchsize):\n",
        "    for j in range(len(valdomainsize)):\n",
        "        if((i+1)*domainbatchsize>valdomainsize[j]):\n",
        "            val_domain_labels.extend(vallabels[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            val_domain_labels.extend(vallabels[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "val_sentiment_labels = []\n",
        "for i in range(max(valdomainsize)//domainbatchsize):\n",
        "    for j in range(len(valdomainsize)):\n",
        "        if((i+1)*domainbatchsize>valdomainsize[j]):\n",
        "            val_sentiment_labels.extend(domains_val_labels[j][i*domainbatchsize:])\n",
        "        else:\n",
        "            val_sentiment_labels.extend(domains_val_labels[j][i*domainbatchsize:(i+1)*domainbatchsize])\n",
        "            \n",
        "val_domain_indices = torch.stack([x for x in val_domain_indices])\n",
        "val_domain_labels = torch.from_numpy(np.asarray(val_domain_labels,'int32'))\n",
        "val_sentiment_labels = torch.from_numpy(np.asarray(val_sentiment_labels,'int32'))"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dK2tC6JdFUWq",
        "colab_type": "code",
        "outputId": "aa0e7d3a-ad9d-4f02-d437-3d5c8275fdc5",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "train_domain_indices.size(),val_domain_indices.size()"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(torch.Size([22180, 300]), torch.Size([3200, 300]))"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lVjzAXmrFUbE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "train_array = torch.utils.data.TensorDataset(train_domain_indices,train_domain_labels,train_sentiment_labels)\n",
        "train_loader = torch.utils.data.DataLoader(train_array,batchsize)\n",
        "\n",
        "val_array = torch.utils.data.TensorDataset(val_domain_indices,val_domain_labels,val_sentiment_labels)\n",
        "val_loader = torch.utils.data.DataLoader(val_array,batchsize)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uL20kP38FUhO",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class DomainClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(DomainClassifier,self).__init__()\n",
        "        self.hiddendim = hiddendim\n",
        "        self.maxlen = maxlen\n",
        "        self.embeddim = embeddim\n",
        "        self.fullyhidden1 = 300\n",
        "        self.numclasses1 = len(traindomainsize)\n",
        "        self.embed = nn.Embedding.from_pretrained(embedding_matrix,freeze=True)\n",
        "        self.lstm = nn.LSTM(self.embeddim,self.hiddendim,batch_first=True,bidirectional=True)\n",
        "        self.fc1 = nn.Linear(self.hiddendim*2,self.fullyhidden1)\n",
        "        self.fc2 = nn.Linear(self.fullyhidden1,self.numclasses1)\n",
        "        self.drop = nn.Dropout(0.5)\n",
        "    \n",
        "    def forward(self,x):\n",
        "        x = self.embed(x)\n",
        "        lstmout,_ = self.lstm(x,None)\n",
        "        out = torch.mean(lstmout,1)\n",
        "        logits = self.drop(F.relu(self.fc1(out)))\n",
        "        logits = self.fc2(logits)\n",
        "        return lstmout,out,logits"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E0cKT4qGFUmi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Attention(nn.Module):\n",
        "    def __init__(self,hidden_dim,attention_dim=embeddim):\n",
        "        super(Attention,self).__init__()\n",
        "        self.maxlen = maxlen\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.attention_dim = attention_dim\n",
        "        self.w = nn.Parameter(torch.from_numpy(np.random.normal(loc=0.0,scale=0.1,size=(hidden_dim,attention_dim)))).float().to(device)\n",
        "        self.b = nn.Parameter(torch.from_numpy(np.random.normal(loc=0.0,scale=0.1,size=attention_dim))).float().to(device)\n",
        "        self.u = nn.Parameter(torch.from_numpy(np.random.normal(loc=0.0,scale=0.1,size=(attention_dim,1)))).float().to(device)\n",
        "        \n",
        "    def forward(self,x):\n",
        "        xlstm = x.view(-1,self.hidden_dim)\n",
        "        v = torch.tanh(torch.mm(xlstm,self.w) + self.b)\n",
        "        vu = torch.mm(v,self.u).view(-1,self.maxlen)\n",
        "        alpha = F.softmax(vu,dim=1).unsqueeze(2)\n",
        "        out = torch.bmm(x.transpose(1,2),alpha).squeeze()\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DU8W5fbCFUqm",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class SentimentClassifier(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(SentimentClassifier,self).__init__()\n",
        "        self.hiddendim = hiddendim\n",
        "        self.maxlen = maxlen\n",
        "        self.embeddim = embeddim\n",
        "        self.fchidden = 300\n",
        "        self.numclasses2 = 2\n",
        "        self.embed = nn.Embedding.from_pretrained(embedding_matrix)\n",
        "        self.lstm = nn.LSTM(self.embeddim+self.hiddendim,self.hiddendim,batch_first=True,bidirectional=True)\n",
        "        self.fclstm = nn.Linear(self.hiddendim*2,self.hiddendim)\n",
        "        self.attention = Attention(self.hiddendim*4,self.hiddendim)\n",
        "        self.fc1 = nn.Linear(self.hiddendim*4,self.fchidden)\n",
        "        self.fc2 = nn.Linear(self.fchidden,self.numclasses2)\n",
        "        \n",
        "    def forward(self,x,out1,out2):\n",
        "        lstmout1 = out1.contiguous().view(-1,self.hiddendim*2)\n",
        "        lstmout1 = self.fclstm(lstmout1).view(out2.size(0),-1,self.hiddendim)\n",
        "        x = self.embed(x)\n",
        "        x = torch.cat([lstmout1,x],2)\n",
        "        a1,_ = self.lstm(x,None)\n",
        "        out2 = out2.unsqueeze(1).repeat(1,self.maxlen,1)\n",
        "        a1 = torch.cat([a1,out2],2)\n",
        "        a1 = self.attention(a1)\n",
        "        a1 = F.dropout(F.relu(self.fc1(a1),1),0.5)\n",
        "        a1 = self.fc2(a1)\n",
        "        return a1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HfccXcJ0FUws",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def domainevaluate(model1,loader):\n",
        "    curloss = 0.0\n",
        "    acc = 0\n",
        "    total = 0\n",
        "    model1.eval()\n",
        "    with torch.no_grad():\n",
        "        for domind,domlab,senlab in loader:\n",
        "            domainind,domlab,senlab = domind.long().to(device),domlab.long().to(device),senlab.long().to(device)\n",
        "            lstmout,out,logits = model1(domainind)\n",
        "            \n",
        "            total+=logits.size(0)\n",
        "            l = F.cross_entropy(logits,domlab,reduction='sum')\n",
        "            curloss+=l.item()\n",
        "            output = torch.max(logits,1)[1]\n",
        "            acc+=torch.sum(domlab==output).item()\n",
        "\n",
        "        return (curloss/total),((acc/total)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "E9SGCl8WFU2H",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sentimentevaluate(model1,model2,loader):\n",
        "    curloss = 0.0\n",
        "    acc = 0\n",
        "    total = 0\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    with torch.no_grad():\n",
        "        for domind,domlab,senlab in loader:\n",
        "            domainind,domlab,senlab = domind.long().to(device),domlab.long().to(device),senlab.long().to(device)\n",
        "            lstmout,mpout,_ = model1(domainind)\n",
        "            output = model2(domainind,lstmout,mpout)\n",
        "            total+=output.size(0)\n",
        "            l = F.cross_entropy(output,senlab,reduction='sum')\n",
        "            curloss+=l.item()\n",
        "            output = torch.max(output,1)[1]\n",
        "            acc+=torch.sum(senlab==output).item()\n",
        "\n",
        "        return (curloss/total),((acc/total)*100)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LI5psIhFUuQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def get_test_acc(vectors,labels,model1,model2):\n",
        "    acc = 0.0\n",
        "    total = 0.0\n",
        "    bs = vectors.size(0)//testbatchsize\n",
        "    model1.eval()\n",
        "    model2.eval()\n",
        "    with torch.no_grad():\n",
        "        for i in range(bs):\n",
        "            if((i+1)*testbatchsize>vectors.size(0)):\n",
        "                vec = vectors[i*testbatchsize:]\n",
        "                lbs = labels[i*testbatchsize:]\n",
        "            else:\n",
        "                vec = vectors[i*testbatchsize:(i+1)*testbatchsize]\n",
        "                lbs = labels[i*testbatchsize:(i+1)*testbatchsize]\n",
        "            total+=vec.size(0)\n",
        "            vec,lbs = vec.long().to(device),lbs.long().to(device)\n",
        "            lstmout,out,logits = model1(vec)\n",
        "            output = model2(vec,lstmout,out)\n",
        "            total+=output.size(0)\n",
        "            output = torch.max(output,1)[1]\n",
        "            acc+=torch.sum(lbs==output).item()\n",
        "\n",
        "        return round((acc/total)*100,3)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1kIiY8cbFifV",
        "colab_type": "code",
        "outputId": "70610a3d-2169-40ba-8cea-3086e07e2db3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 765
        }
      },
      "source": [
        "lamda_d = 0.5\n",
        "lamda_s = 1.0\n",
        "patience = 10\n",
        "curpatience = patience\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "domainmodel = DomainClassifier().to(device)\n",
        "sentimentmodel = SentimentClassifier().to(device)\n",
        "\n",
        "optim1 = torch.optim.Adam(domainmodel.parameters(),lr=0.004)\n",
        "optim2 = torch.optim.Adam(sentimentmodel.parameters(),lr=0.004)\n",
        "\n",
        "scheduler1 = torch.optim.lr_scheduler.StepLR(optim1, 100, gamma=0.1, last_epoch=-1)\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(optim2, 100, gamma=0.1, last_epoch=-1)\n",
        "\n",
        "best_domainmodel_wts = deepcopy(domainmodel.state_dict())\n",
        "best_sentimentmodel_wts = deepcopy(sentimentmodel.state_dict())\n",
        "\n",
        "print(\"---------Training Domain Classifier-----------------\")\n",
        "epochs1 = 100\n",
        "domainmodel.train()\n",
        "sentimentmodel.train()\n",
        "bestdomainloss = np.Inf\n",
        "for epoch in range(1,epochs1+1):\n",
        "    domainmodel.train()\n",
        "    sentimentmodel.train()\n",
        "    for domind,domlab,senlab in train_loader:\n",
        "        domainmodel.zero_grad()\n",
        "        sentimentmodel.zero_grad()\n",
        "        domainind,domlab,senlab = domind.long().to(device),domlab.long().to(device),senlab.long().to(device)\n",
        "        a1,a2,logits = domainmodel(domainind)\n",
        "        output = sentimentmodel(domainind,a1,a2)\n",
        "        loss1 = F.cross_entropy(logits,domlab)\n",
        "        loss2 = F.cross_entropy(output,senlab)\n",
        "        loss_temp = lamda_d * loss1 + lamda_s * loss2\n",
        "        loss_temp.backward()\n",
        "        optim1.step()\n",
        "        optim2.step()\n",
        "\n",
        "    scheduler1.step()\n",
        "    scheduler2.step()\n",
        "\n",
        "    curdomaintrainloss,curdomaintrainacc = domainevaluate(domainmodel,train_loader)\n",
        "    curdomainvalloss,curdomainvalacc = domainevaluate(domainmodel,val_loader)\n",
        "    #if(epoch%10==0):\n",
        "    print(\"Epoch {} Train Loss {} Train Accuracy {} \".format(epoch,curdomaintrainloss,curdomaintrainacc))\n",
        "    print(\"Validation Loss {} Validation Accuracy {} \".format(curdomainvalloss,curdomainvalacc))\n",
        "    print(\"-----------------------------------------------------------------------------------\")\n",
        "\n",
        "    if(curdomainvalloss<bestdomainloss):\n",
        "        bestdomainloss = curdomainvalloss\n",
        "        best_domainmodel_wts = deepcopy(domainmodel.state_dict())\n",
        "        best_sentimentmodel_wts = deepcopy(sentimentmodel.state_dict())\n",
        "        curpatience = patience\n",
        "    else:\n",
        "      curpatience = curpatience - 1\n",
        "      if(curpatience==0):\n",
        "        break\n",
        " \n",
        "domainmodel = DomainClassifier().to(device)\n",
        "sentimentmodel = SentimentClassifier().to(device)\n",
        "domainmodel.load_state_dict(best_domainmodel_wts)\n",
        "sentimentmodel.load_state_dict(best_sentimentmodel_wts)"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------Training Domain Classifier-----------------\n",
            "Epoch 1 Train Loss 0.8895491048922078 Train Accuracy 68.10640216411181 \n",
            "Validation Loss 0.9768868735432625 Validation Accuracy 66.21875 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 2 Train Loss 0.7532550389106688 Train Accuracy 72.15058611361587 \n",
            "Validation Loss 0.9560525470972061 Validation Accuracy 67.1875 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 3 Train Loss 0.6315838692583836 Train Accuracy 76.08205590622183 \n",
            "Validation Loss 0.9174105374515057 Validation Accuracy 69.25 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 4 Train Loss 0.5102433297470736 Train Accuracy 81.28944995491435 \n",
            "Validation Loss 0.8990827013552188 Validation Accuracy 71.375 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 5 Train Loss 0.45764198553142943 Train Accuracy 83.11541929666366 \n",
            "Validation Loss 0.976252950578928 Validation Accuracy 70.75 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 6 Train Loss 0.42666127532317466 Train Accuracy 84.39134355275023 \n",
            "Validation Loss 1.0498792643100023 Validation Accuracy 71.46875 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 7 Train Loss 0.366890222429423 Train Accuracy 86.52840396753832 \n",
            "Validation Loss 1.0140686885267496 Validation Accuracy 71.90625 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 8 Train Loss 0.33690021860642516 Train Accuracy 87.7502254283138 \n",
            "Validation Loss 1.0545705474168061 Validation Accuracy 71.625 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 9 Train Loss 0.31762881833653495 Train Accuracy 88.80522993688007 \n",
            "Validation Loss 1.1209557738900184 Validation Accuracy 73.25 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 10 Train Loss 0.32955759078452346 Train Accuracy 88.25969341749324 \n",
            "Validation Loss 1.2025048147141932 Validation Accuracy 71.78125 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 11 Train Loss 0.27619038642903937 Train Accuracy 90.23444544634806 \n",
            "Validation Loss 1.2125294985994697 Validation Accuracy 73.0625 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 12 Train Loss 0.26764645269048226 Train Accuracy 90.37871956717764 \n",
            "Validation Loss 1.252136217840016 Validation Accuracy 73.0625 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 13 Train Loss 0.26209117574961577 Train Accuracy 90.51397655545537 \n",
            "Validation Loss 1.232822760231793 Validation Accuracy 71.96875 \n",
            "-----------------------------------------------------------------------------------\n",
            "Epoch 14 Train Loss 0.24011979264596006 Train Accuracy 91.33002705139765 \n",
            "Validation Loss 1.2413860341906549 Validation Accuracy 72.53125 \n",
            "-----------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ChXhu8NwFizh",
        "colab_type": "code",
        "outputId": "51c35a98-1021-43b2-f296-b899ea8310e1",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "print(\"---------Training sentiment Classifier-----------------\")\n",
        "epochs2 = 500\n",
        "\n",
        "random.seed(seed)\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "optim2 = torch.optim.Adam(sentimentmodel.parameters(),lr=0.004)\n",
        "scheduler2 = torch.optim.lr_scheduler.StepLR(optim2, 100, gamma=0.1, last_epoch=-1)\n",
        "\n",
        "patience = 50\n",
        "curpatience = patience\n",
        "bestsentimentloss = np.Inf\n",
        "best_sentimentmodel_wts = deepcopy(sentimentmodel.state_dict())\n",
        "sentimentmodel.train()\n",
        "domainmodel.eval()\n",
        "for epoch in range(1,epochs2+1):\n",
        "    sentimentmodel.train()\n",
        "    for domind,domlab,senlab in train_loader:\n",
        "        domainind,domlab,senlab = domind.long().to(device),domlab.long().to(device),senlab.long().to(device)\n",
        "        with torch.no_grad():\n",
        "          lstmoutput,mpoutput,_ = domainmodel(domainind)\n",
        "        pred = sentimentmodel(domainind,lstmoutput,mpoutput)\n",
        "        loss2 =  F.cross_entropy(pred,senlab)\n",
        "        lossnew = lamda_s * loss2\n",
        "        sentimentmodel.zero_grad()\n",
        "        lossnew.backward()\n",
        "        optim2.step()\n",
        "\n",
        "    scheduler2.step()\n",
        "    cursentimenttrainloss,cursentimenttrainacc = sentimentevaluate(domainmodel,sentimentmodel,train_loader)\n",
        "    cursentimentvalloss,cursentimentvalacc = sentimentevaluate(domainmodel,sentimentmodel,val_loader)\n",
        "    print(\"Epoch {} Train Loss {} Train Accuracy {} \".format(epoch,cursentimenttrainloss,cursentimenttrainacc))\n",
        "    print(\"Validation Loss {} Validation Accuracy {} \".format(cursentimentvalloss,cursentimentvalacc))\n",
        "    print(\"-----------------------------------------------------------------------------------------\")\n",
        "    #if(epoch%10==0):\n",
        "      #print(\"Epoch {} Loss {} \".format(epoch,cursentimentvalloss))\n",
        "    if(cursentimentvalloss<bestsentimentloss):\n",
        "        bestsentimentloss = cursentimentvalloss\n",
        "        best_sentimentmodel_wts = deepcopy(sentimentmodel.state_dict())\n",
        "        curpatience = patience\n",
        "    else:\n",
        "      curpatience = curpatience - 1\n",
        "      if(curpatience==0):\n",
        "        break\n",
        "  \n",
        "sentimentmodel = SentimentClassifier().to(device)\n",
        "sentimentmodel.load_state_dict(best_sentimentmodel_wts)"
      ],
      "execution_count": 44,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "---------Training sentiment Classifier-----------------\n",
            "Epoch 1 Train Loss 0.2799194816402533 Train Accuracy 89.66185752930568 \n",
            "Validation Loss 0.6848115152865648 Validation Accuracy 83.21875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 2 Train Loss 0.2761978342914173 Train Accuracy 89.7385031559964 \n",
            "Validation Loss 0.6923473070561886 Validation Accuracy 83.25 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 3 Train Loss 0.27842604858419506 Train Accuracy 89.78358881875563 \n",
            "Validation Loss 0.7378285152465105 Validation Accuracy 82.75 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 4 Train Loss 0.2781976039542391 Train Accuracy 89.85572587917042 \n",
            "Validation Loss 0.7466972509585321 Validation Accuracy 82.25 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 5 Train Loss 0.27055331486855894 Train Accuracy 90.02705139765554 \n",
            "Validation Loss 0.740369858033955 Validation Accuracy 83.28125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 6 Train Loss 0.2649894175938489 Train Accuracy 90.21190261496844 \n",
            "Validation Loss 0.7558396384306252 Validation Accuracy 82.96875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 7 Train Loss 0.2735298980776526 Train Accuracy 90.08566275924255 \n",
            "Validation Loss 0.8109763910993933 Validation Accuracy 82.8125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 8 Train Loss 0.2611479065123176 Train Accuracy 90.23895401262398 \n",
            "Validation Loss 0.7705222328938544 Validation Accuracy 82.75 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 9 Train Loss 0.2648076637598499 Train Accuracy 90.04508566275923 \n",
            "Validation Loss 0.8413199329562485 Validation Accuracy 82.90625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 10 Train Loss 0.2584629610772064 Train Accuracy 90.41027953110911 \n",
            "Validation Loss 0.7955160567723215 Validation Accuracy 82.875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 11 Train Loss 0.2517313272446206 Train Accuracy 90.43282236248874 \n",
            "Validation Loss 0.8248461569473148 Validation Accuracy 83.03125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 12 Train Loss 0.25458026057003424 Train Accuracy 90.52750225428315 \n",
            "Validation Loss 0.8501454091444611 Validation Accuracy 83.0625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 13 Train Loss 0.24951126056415096 Train Accuracy 90.9467989179441 \n",
            "Validation Loss 0.8491129411570728 Validation Accuracy 82.4375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 14 Train Loss 0.2475527528011659 Train Accuracy 90.78449053201082 \n",
            "Validation Loss 0.8720680206827819 Validation Accuracy 82.5625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 15 Train Loss 0.24752379326373206 Train Accuracy 90.9287646528404 \n",
            "Validation Loss 0.9448127137124538 Validation Accuracy 81.96875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 16 Train Loss 0.23801234069173674 Train Accuracy 91.12263300270514 \n",
            "Validation Loss 0.8807606304064393 Validation Accuracy 83.0 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 17 Train Loss 0.2305809197404881 Train Accuracy 91.28043282236249 \n",
            "Validation Loss 0.954123972915113 Validation Accuracy 83.0 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 18 Train Loss 0.2383609569051225 Train Accuracy 91.32551848512172 \n",
            "Validation Loss 1.0001177414692939 Validation Accuracy 82.4375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 19 Train Loss 0.235175792734509 Train Accuracy 91.31199278629396 \n",
            "Validation Loss 0.9559671707078814 Validation Accuracy 82.8125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 20 Train Loss 0.22688134733463214 Train Accuracy 91.51487826871055 \n",
            "Validation Loss 0.9989774533547461 Validation Accuracy 82.84375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 21 Train Loss 0.23103322487515948 Train Accuracy 91.42470694319206 \n",
            "Validation Loss 0.9845016274787486 Validation Accuracy 82.875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 22 Train Loss 0.21685654553236244 Train Accuracy 91.81695220919748 \n",
            "Validation Loss 0.9317150372639298 Validation Accuracy 82.9375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 23 Train Loss 0.21923200685876892 Train Accuracy 91.70423805229937 \n",
            "Validation Loss 1.048615191485733 Validation Accuracy 82.15625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 24 Train Loss 0.21829992453454314 Train Accuracy 91.75834084761047 \n",
            "Validation Loss 1.0337703202664852 Validation Accuracy 83.1875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 25 Train Loss 0.21907191207620666 Train Accuracy 91.78990081154193 \n",
            "Validation Loss 1.0359081122651697 Validation Accuracy 82.53125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 26 Train Loss 0.2111895234454205 Train Accuracy 92.1370604147881 \n",
            "Validation Loss 1.0052216319367289 Validation Accuracy 82.28125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 27 Train Loss 0.21290583878459965 Train Accuracy 91.99278629395852 \n",
            "Validation Loss 1.0735656477510929 Validation Accuracy 81.78125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 28 Train Loss 0.21153380383608897 Train Accuracy 92.10099188458071 \n",
            "Validation Loss 1.1115615189447998 Validation Accuracy 82.34375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 29 Train Loss 0.20804756393402518 Train Accuracy 92.4346257889991 \n",
            "Validation Loss 1.1371513614617288 Validation Accuracy 82.46875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 30 Train Loss 0.2090505557112913 Train Accuracy 92.40306582506763 \n",
            "Validation Loss 1.1859155776910484 Validation Accuracy 82.3125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 31 Train Loss 0.20550151431582014 Train Accuracy 92.47971145175833 \n",
            "Validation Loss 1.2086345238983631 Validation Accuracy 82.53125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 32 Train Loss 0.19660844562879223 Train Accuracy 92.55635707844905 \n",
            "Validation Loss 1.1170787084102631 Validation Accuracy 82.09375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 33 Train Loss 0.2103273987501349 Train Accuracy 92.36699729486023 \n",
            "Validation Loss 1.213015900682658 Validation Accuracy 82.28125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 34 Train Loss 0.2055292713671681 Train Accuracy 92.46167718665464 \n",
            "Validation Loss 1.2742907354049384 Validation Accuracy 81.625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 35 Train Loss 0.21477590016919929 Train Accuracy 92.17312894499548 \n",
            "Validation Loss 1.395089096147567 Validation Accuracy 81.4375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 36 Train Loss 0.19921883702197518 Train Accuracy 92.52028854824165 \n",
            "Validation Loss 1.2960004323534668 Validation Accuracy 82.21875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 37 Train Loss 0.19250053309367732 Train Accuracy 92.83137962128043 \n",
            "Validation Loss 1.3972023897245527 Validation Accuracy 81.75 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 38 Train Loss 0.19018756804754328 Train Accuracy 92.87195671776375 \n",
            "Validation Loss 1.3664890520647168 Validation Accuracy 82.21875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 39 Train Loss 0.18224139897349292 Train Accuracy 92.93958521190262 \n",
            "Validation Loss 1.327101908158511 Validation Accuracy 81.96875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 40 Train Loss 0.1881526637617482 Train Accuracy 93.0117222723174 \n",
            "Validation Loss 1.3088100141100585 Validation Accuracy 82.78125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 41 Train Loss 0.19166902698059143 Train Accuracy 92.99819657348964 \n",
            "Validation Loss 1.3792043993249536 Validation Accuracy 82.625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 42 Train Loss 0.19318746686438595 Train Accuracy 92.87646528403968 \n",
            "Validation Loss 1.5565452378802 Validation Accuracy 81.3125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 43 Train Loss 0.1830670853312538 Train Accuracy 93.20559062218214 \n",
            "Validation Loss 1.484037391319871 Validation Accuracy 82.4375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 44 Train Loss 0.1729104395838231 Train Accuracy 93.22362488728584 \n",
            "Validation Loss 1.414708074554801 Validation Accuracy 82.125 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 45 Train Loss 0.17924431442300728 Train Accuracy 93.00721370604148 \n",
            "Validation Loss 1.4608650870621205 Validation Accuracy 81.875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 46 Train Loss 0.18283497643589006 Train Accuracy 93.07935076645627 \n",
            "Validation Loss 1.578149097673595 Validation Accuracy 81.46875 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 47 Train Loss 0.180333408237686 Train Accuracy 93.39495040577096 \n",
            "Validation Loss 1.408669832777232 Validation Accuracy 81.9375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 48 Train Loss 0.1793682963542835 Train Accuracy 93.38593327321911 \n",
            "Validation Loss 1.5814057405665516 Validation Accuracy 81.5625 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 49 Train Loss 0.1730360523847119 Train Accuracy 93.50766456266906 \n",
            "Validation Loss 1.4711985085066408 Validation Accuracy 81.375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 50 Train Loss 0.18032560997362929 Train Accuracy 93.273219116321 \n",
            "Validation Loss 1.5758752049133182 Validation Accuracy 82.09375 \n",
            "-----------------------------------------------------------------------------------------\n",
            "Epoch 51 Train Loss 0.17236169684003128 Train Accuracy 93.48061316501352 \n",
            "Validation Loss 1.666142854038626 Validation Accuracy 82.28125 \n",
            "-----------------------------------------------------------------------------------------\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 44
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "44RwkQZ1FpWg",
        "colab_type": "code",
        "outputId": "94a810ae-9f7e-4732-8702-ebaf9241fa1f",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        }
      },
      "source": [
        "for domain in domains:\n",
        "  test = pd.read_csv(datapath+domain+'/test.csv')\n",
        "  domains_test_labels = torch.from_numpy(test['label'].astype(int).values)\n",
        "  domains_test_corpus = preprocess(list(test['text']))\n",
        "  domains_test_corpus = get_data(domains_test_corpus,vocabulary)\n",
        "  acc = get_test_acc(domains_test_corpus,domains_test_labels,domainmodel,sentimentmodel)\n",
        "  print(\"Domain {} ---> Test Accuracy {} \".format(domain[0].upper()+domain[1:],acc))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Domain Music ---> Test Accuracy 40.234 \n",
            "Domain Mr ---> Test Accuracy 36.849 \n",
            "Domain Health_personal_care ---> Test Accuracy 40.625 \n",
            "Domain Kitchen_housewares ---> Test Accuracy 39.844 \n",
            "Domain Software ---> Test Accuracy 41.406 \n",
            "Domain Sports_outdoors ---> Test Accuracy 39.974 \n",
            "Domain Imdb ---> Test Accuracy 43.62 \n",
            "Domain Toys_games ---> Test Accuracy 41.146 \n",
            "Domain Video ---> Test Accuracy 43.359 \n",
            "Domain Magazines ---> Test Accuracy 42.969 \n",
            "Domain Books ---> Test Accuracy 40.755 \n",
            "Domain Apparel ---> Test Accuracy 41.276 \n",
            "Domain Baby ---> Test Accuracy 40.625 \n",
            "Domain Camera_photo ---> Test Accuracy 42.839 \n",
            "Domain Electronics ---> Test Accuracy 40.104 \n",
            "Domain Dvd ---> Test Accuracy 41.927 \n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RS-VWxCMy3kQ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}
